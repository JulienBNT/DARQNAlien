{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D, Layer\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU (askip)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) disponible(s): {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"CPU utilisé\")\n",
    "\n",
    "ENV_NAME = \"ALE/Alien-v5\"\n",
    "LEARNING_RATE = 0.00025\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 0.995\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "EPISODES = 500\n",
    "UPDATE_TARGET_FREQ = 10\n",
    "FRAME_STACK = 4\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54683d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(Layer):\n",
    "    def __init__(self, ratio=8, kernel_size=7, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = int(input_shape[-1])\n",
    "        hidden = max(channels // self.ratio, 1)\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.gmp = GlobalMaxPooling2D()\n",
    "        self.fc1 = Dense(hidden, activation='relu', kernel_initializer='he_normal', use_bias=True)\n",
    "        self.fc2 = Dense(channels, activation=None, kernel_initializer='he_normal', use_bias=True)\n",
    "        self.spatial_conv = Conv2D(filters=1, kernel_size=self.kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        avg_pool = self.fc2(self.fc1(self.gap(inputs)))\n",
    "        max_pool = self.fc2(self.fc1(self.gmp(inputs)))\n",
    "        channel_attn = tf.nn.sigmoid(avg_pool + max_pool)\n",
    "        channel_attn = tf.reshape(channel_attn, (-1, 1, 1, tf.shape(inputs)[-1]))\n",
    "        x = inputs * channel_attn\n",
    "        avg_spatial = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        max_spatial = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "        spatial = tf.concat([avg_spatial, max_spatial], axis=-1)\n",
    "        spatial_attn = self.spatial_conv(spatial)\n",
    "        return x * spatial_attn\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"ratio\": self.ratio, \"kernel_size\": self.kernel_size})\n",
    "        return cfg\n",
    "\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = tf.image.rgb_to_grayscale(frame)\n",
    "    frame = tf.image.resize(frame, [84, 84])\n",
    "    frame = tf.cast(frame, tf.float32) / 255.0\n",
    "    return frame\n",
    "\n",
    "\n",
    "def build_darqn_model(input_shape, n_actions):\n",
    "    state_input = layers.Input(shape=input_shape, name='state_input')\n",
    "    \n",
    "    x = Conv2D(32, (8, 8), strides=4, activation='relu', kernel_initializer='he_normal')(state_input)\n",
    "    x = CBAM(ratio=8, kernel_size=7)(x)\n",
    "    \n",
    "    x = Conv2D(64, (4, 4), strides=2, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = CBAM(ratio=8, kernel_size=7)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=1, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = CBAM(ratio=8, kernel_size=7)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = layers.Reshape((FRAME_STACK, -1))(x)\n",
    "    \n",
    "    x = layers.LSTM(LSTM_UNITS, activation='tanh', return_sequences=False)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    output = Dense(n_actions, activation='linear', kernel_initializer='he_normal', name='q_values')(x)\n",
    "    \n",
    "    model = Model(inputs=state_input, outputs=output, name='DARQN_CBAM')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='huber',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), \n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DARQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        self.model = build_darqn_model(state_shape, n_actions)\n",
    "        self.target_model = build_darqn_model(state_shape, n_actions)\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        self.frame_stack = deque(maxlen=FRAME_STACK)\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def train(self):\n",
    "        if self.replay_buffer.size() < BATCH_SIZE:\n",
    "            return 0.0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        max_next_q_values = np.max(next_q_values, axis=1)\n",
    "        targets = rewards + GAMMA * max_next_q_values * (1 - dones)\n",
    "        \n",
    "        target_q_values = self.model.predict(states, verbose=0)\n",
    "        for i, action in enumerate(actions):\n",
    "            target_q_values[i][action] = targets[i]\n",
    "        \n",
    "        loss = self.model.train_on_batch(states, target_q_values)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(EPSILON_MIN, self.epsilon * EPSILON_DECAY)\n",
    "    \n",
    "    def stack_frames(self, frame, is_new_episode=False):\n",
    "        frame = preprocess_frame(frame)\n",
    "        \n",
    "        if is_new_episode:\n",
    "            self.frame_stack.clear()\n",
    "            for _ in range(FRAME_STACK):\n",
    "                self.frame_stack.append(frame)\n",
    "        else:\n",
    "            self.frame_stack.append(frame)\n",
    "        \n",
    "        stacked = np.stack(self.frame_stack, axis=-1)\n",
    "        return stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode=None)\n",
    "n_actions = env.action_space.n\n",
    "state_shape = (84, 84, FRAME_STACK)\n",
    "\n",
    "agent = DARQNAgent(state_shape, n_actions)\n",
    "\n",
    "agent.model.summary()\n",
    "\n",
    "print(f\"{n_actions}\")\n",
    "print(f\"{state_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd002c",
   "metadata": {},
   "source": [
    "entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd11e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_darqn():\n",
    "    log_dir = f\"logs/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Début de l'entraînement - {EPISODES} épisodes\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        state = agent.stack_frames(state, is_new_episode=True)\n",
    "        \n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = agent.stack_frames(next_state)\n",
    "            \n",
    "            agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            loss = agent.train()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_loss += loss\n",
    "            steps += 1\n",
    "        \n",
    "        if episode % UPDATE_TARGET_FREQ == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        avg_loss = total_loss / steps if steps > 0 else 0\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            os.makedirs('models', exist_ok=True)\n",
    "            agent.model.save('models/best_darqn_cbam_alien.keras')\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_reward_10 = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "            print(f\"Épisode {episode:3d} | \"\n",
    "                  f\"Récompense: {total_reward:6.1f} | \"\n",
    "                  f\"Moyenne (10): {avg_reward_10:6.1f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Steps: {steps:4d} | \"\n",
    "                  f\"Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if episode % 50 == 0 and episode > 0:\n",
    "            os.makedirs('models', exist_ok=True)\n",
    "            agent.model.save(f'models/darqn_cbam_alien_ep{episode}.keras')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Entraînement terminé!\")\n",
    "    print(f\"Meilleure récompense: {best_reward:.1f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return episode_rewards, episode_losses\n",
    "\n",
    "rewards, losses = train_darqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2471e3",
   "metadata": {},
   "source": [
    "graphes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651bb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(rewards, losses):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    axes[0].plot(rewards, alpha=0.6, label='Récompense par épisode')\n",
    "    if len(rewards) >= 10:\n",
    "        moving_avg = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
    "        axes[0].plot(range(9, len(rewards)), moving_avg, 'r-', linewidth=2, label='Moyenne mobile (10)')\n",
    "    axes[0].set_xlabel('Épisode')\n",
    "    axes[0].set_ylabel('Récompense totale')\n",
    "    axes[0].set_title('Évolution des récompenses au cours de l\\'entraînement')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(losses, alpha=0.6, color='orange', label='Loss par épisode')\n",
    "    if len(losses) >= 10:\n",
    "        moving_avg_loss = np.convolve(losses, np.ones(10)/10, mode='valid')\n",
    "        axes[1].plot(range(9, len(losses)), moving_avg_loss, 'r-', linewidth=2, label='Moyenne mobile (10)')\n",
    "    axes[1].set_xlabel('Épisode')\n",
    "    axes[1].set_ylabel('Loss moyenne')\n",
    "    axes[1].set_title('Évolution de la loss au cours de l\\'entraînement')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results_darqn_cbam.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nrécompenses:\")\n",
    "    print(f\"moyenne {np.mean(rewards):.2f}\")\n",
    "    print(f\"max {np.max(rewards):.2f}\")\n",
    "    print(f\"min {np.min(rewards):.2f}\")\n",
    "    print(f\"Loss moyenne: {np.mean(losses):.4f}\")\n",
    "\n",
    "plot_training_results(rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011de7b",
   "metadata": {},
   "source": [
    "test de l'agent entrainé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc53ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_agent(model_path='models/best_darqn_cbam_alien.keras', n_episodes=5, render=True):\n",
    "    test_agent = DARQNAgent(state_shape, n_actions)\n",
    "    test_agent.model = keras.models.load_model(model_path, custom_objects={'CBAM': CBAM})\n",
    "    test_agent.epsilon = 0.0\n",
    "    \n",
    "    render_mode = 'human' if render else None\n",
    "    test_env = gym.make(ENV_NAME, render_mode=render_mode)\n",
    "    \n",
    "    test_rewards = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test de l'agent sur {n_episodes} épisodes\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = test_env.reset()\n",
    "        state = test_agent.stack_frames(state, is_new_episode=True)\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = test_agent.get_action(state, training=False)\n",
    "            next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_state = test_agent.stack_frames(next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        test_rewards.append(total_reward)\n",
    "        print(f\"Épisode {episode + 1}: Récompense = {total_reward:.1f}, Steps = {steps}\")\n",
    "    \n",
    "    test_env.close()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  - Récompense moyenne: {np.mean(test_rewards):.2f}\")\n",
    "    print(f\"  - Récompense max: {np.max(test_rewards):.2f}\")\n",
    "    print(f\"  - Récompense min: {np.min(test_rewards):.2f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return test_rewards\n",
    "\n",
    "# test de l'agent après l'entraînement (faut décommenter)\n",
    "# test_rewards = test_agent(render=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
